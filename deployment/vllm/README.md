# vLLM

vLLM is a high-performance, easy-to-use engine for LLM inference and serving. For an in-depth overview, see the [documentation](https://docs.vllm.ai/).

This directory contains vLLM deployment guide for MiniCPM-o 4.5.

## Deployment Guide

- MiniCPM-o 4.5: [English](./minicpm-o4_5_vllm.md) | [中文](./minicpm-o4_5_vllm_zh.md)
