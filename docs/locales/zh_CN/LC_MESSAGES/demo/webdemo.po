# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, OpenBMB
# This file is distributed under the same license as the MiniCPM-o Cookbook
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: MiniCPM-o Cookbook \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-07-17 20:35+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/demo/webdemo.md:1
msgid "Omni Stream"
msgstr "全模态流式演示"

#: ../../source/demo/webdemo.md:3
msgid "Overview"
msgstr "概述"

#: ../../source/demo/webdemo.md:5
msgid ""
"The MiniCPM-o 4.5 Omni Stream demo provides a real-time conversational AI "
"experience with multimodal capabilities. It supports full-duplex streaming "
"audio/video input/output, voice activity detection (VAD), voice cloning, and "
"multimodal interactions."
msgstr ""
"MiniCPM-o 4.5 全模态流式演示提供了具备多模态能力的实时对话式人工智能体验。"
"它支持全双工流式音视频输入/输出、语音活动检测（VAD）、声音克隆以及多模态交互。"

#: ../../source/demo/webdemo.md:7
msgid "Key Features"
msgstr "主要特性"

#: ../../source/demo/webdemo.md:9
msgid "Full-Duplex Real-time Streaming"
msgstr "全双工实时流式处理"

#: ../../source/demo/webdemo.md:10
msgid "**Streaming Audio/Video Processing**: Process audio and video input in real-time"
msgstr "**流式音视频处理**：实时处理音视频输入"

#: ../../source/demo/webdemo.md:11
msgid "**Low-latency Response**: Optimized for real-time conversation scenarios"
msgstr "**低延迟响应**：为实时对话场景优化"

#: ../../source/demo/webdemo.md:12
msgid "**Full-Duplex Communication**: Simultaneous bidirectional audio streaming"
msgstr "**全双工通信**：同时双向音频流传输"

#: ../../source/demo/webdemo.md:14
msgid "Voice Activity Detection (VAD)"
msgstr "语音活动检测（VAD）"

#: ../../source/demo/webdemo.md:15
msgid ""
"**Silero VAD Integration**: Automatic speech detection using pre-trained "
"VAD models"
msgstr "**集成 Silero VAD**：使用预训练的 VAD 模型自动检测语音"

#: ../../source/demo/webdemo.md:16
msgid ""
"**Configurable Thresholds**: Customizable silence/speech detection "
"parameters"
msgstr "**可配置阈值**：可自定义的静音/语音检测参数"

#: ../../source/demo/webdemo.md:17
msgid ""
"**Smart Segmentation**: Automatic audio segmentation based on speech "
"patterns"
msgstr "**智能分段**：根据语音模式自动进行音频分段"

#: ../../source/demo/webdemo.md:19
msgid "Voice Cloning & TTS"
msgstr "声音克隆与文本转语音（TTS）"

#: ../../source/demo/webdemo.md:20
msgid ""
"**Reference Audio Support**: Clone voice characteristics from reference "
"audio samples"
msgstr "**支持参考音频**：从参考音频样本中克隆声音特征"

#: ../../source/demo/webdemo.md:21
msgid ""
"**Multiple Voice Options**: Built-in male, female, and default voice "
"presets"
msgstr "**多种声音选项**：内置男性、女性和默认声音预设"

#: ../../source/demo/webdemo.md:22
msgid "**Custom Audio Upload**: Upload your own reference audio for voice cloning"
msgstr "**自定义音频上传**：上传您自己的参考音频进行声音克隆"

#: ../../source/demo/webdemo.md:24
msgid "Multimodal Support"
msgstr "多模态支持"

#: ../../source/demo/webdemo.md:25
msgid ""
"**Audio + Video Input**: Process both audio and visual information "
"simultaneously"
msgstr "**音频 + 视频输入**：同时处理音频和视觉信息"

#: ../../source/demo/webdemo.md:26
msgid "**Text + Audio Output**: Generate both speech and text responses"
msgstr "**文本 + 音频输出**：生成语音和文本两种响应"

#: ../../source/demo/webdemo.md:27
msgid "**High-definition Video**: Support for HD video processing"
msgstr "**高清视频**：支持高清视频处理"

#: ../../source/demo/webdemo.md:29
msgid "Architecture"
msgstr "架构"

#: ../../source/demo/webdemo.md:41
msgid "WebRTC Demo"
msgstr "WebRTC 演示"

#: ../../source/demo/webdemo.md:43
msgid ""
"For MiniCPM-o 4.5, we provide a WebRTC-based full-duplex real-time video "
"interaction demo."
msgstr "对于 MiniCPM-o 4.5，我们提供了基于 WebRTC 的全双工实时视频交互演示。"

#: ../../source/demo/webdemo.md:45
msgid "Prerequisites"
msgstr "前提条件"

#: ../../source/demo/webdemo.md:47
msgid "1. **Install Docker Desktop** (macOS)"
msgstr "1. **安装 Docker Desktop** (macOS)"

#: ../../source/demo/webdemo.md:59
msgid "2. **Build llamacpp-omni Inference Service**"
msgstr "2. **构建 llamacpp-omni 推理服务**"

#: ../../source/demo/webdemo.md:75
msgid "3. **Prepare GGUF Model Files**"
msgstr "3. **准备 GGUF 模型文件**"

#: ../../source/demo/webdemo.md:93
msgid "Quick Start"
msgstr "快速开始"

#: ../../source/demo/webdemo.md:95
msgid ""
"We provide a pre-built Docker image for quick deployment and experience. "
"The Docker image includes all necessary dependencies and configurations."
msgstr "我们提供预构建的 Docker 镜像用于快速部署和体验。Docker 镜像包含所有必要的依赖和配置。"

#: ../../source/demo/webdemo.md:97
msgid "macOS (Apple Silicon)"
msgstr "macOS (Apple Silicon)"

#: ../../source/demo/webdemo.md:99
msgid ""
"**Requirements**: Apple Silicon Mac (M1/M2/M3/M4), **M4 recommended** for "
"optimal performance."
msgstr "**要求**：Apple Silicon Mac (M1/M2/M3/M4)，推荐使用 **M4** 以获得最佳性能。"

#: ../../source/demo/webdemo.md:101
msgid "Download the Docker image for macOS:"
msgstr "下载 macOS 的 Docker 镜像："

#: ../../source/demo/webdemo.md:105
msgid "Deployment Steps"
msgstr "部署步骤"

#: ../../source/demo/webdemo.md:107
msgid "**Step 1: Extract and Load Docker Images**"
msgstr "**步骤 1：解压并加载 Docker 镜像**"

#: ../../source/demo/webdemo.md:121
msgid "**Step 2: Install Python Dependencies**"
msgstr "**步骤 2：安装 Python 依赖**"

#: ../../source/demo/webdemo.md:127
msgid "**Step 3: One-Click Deployment (Recommended)**"
msgstr "**步骤 3：一键部署（推荐）**"

#: ../../source/demo/webdemo.md:149
msgid "The script automatically:"
msgstr "该脚本会自动："

#: ../../source/demo/webdemo.md:151
msgid "Checks Docker environment"
msgstr "检查 Docker 环境"

#: ../../source/demo/webdemo.md:152
msgid "Updates LiveKit configuration with local IP"
msgstr "使用本地 IP 更新 LiveKit 配置"

#: ../../source/demo/webdemo.md:153
msgid "Starts Docker services (frontend, backend, LiveKit, Redis)"
msgstr "启动 Docker 服务（前端、后端、LiveKit、Redis）"

#: ../../source/demo/webdemo.md:154
msgid "Installs Python dependencies"
msgstr "安装 Python 依赖"

#: ../../source/demo/webdemo.md:155
msgid "Starts C++ inference service"
msgstr "启动 C++ 推理服务"

#: ../../source/demo/webdemo.md:156
msgid "Registers inference service to backend"
msgstr "将推理服务注册到后端"

#: ../../source/demo/webdemo.md:158
msgid "**Step 4: Access the Web Interface**"
msgstr "**步骤 4：访问 Web 界面**"

#: ../../source/demo/webdemo.md:168
msgid "Service Ports"
msgstr "服务端口"

#: ../../source/demo/webdemo.md:170
msgid "Service"
msgstr "服务"

#: ../../source/demo/webdemo.md:170
msgid "Port"
msgstr "端口"

#: ../../source/demo/webdemo.md:170
msgid "Description"
msgstr "描述"

#: ../../source/demo/webdemo.md:171
msgid "Frontend"
msgstr "前端"

#: ../../source/demo/webdemo.md:171
msgid "3000"
msgstr "3000"

#: ../../source/demo/webdemo.md:171
msgid "Web UI"
msgstr "Web 界面"

#: ../../source/demo/webdemo.md:172
msgid "Backend"
msgstr "后端"

#: ../../source/demo/webdemo.md:172
msgid "8021"
msgstr "8021"

#: ../../source/demo/webdemo.md:172
msgid "Backend API"
msgstr "后端 API"

#: ../../source/demo/webdemo.md:173
msgid "LiveKit"
msgstr "LiveKit"

#: ../../source/demo/webdemo.md:173
msgid "7880"
msgstr "7880"

#: ../../source/demo/webdemo.md:173
msgid "Real-time communication"
msgstr "实时通信"

#: ../../source/demo/webdemo.md:174
msgid "Inference"
msgstr "推理"

#: ../../source/demo/webdemo.md:174
msgid "9060"
msgstr "9060"

#: ../../source/demo/webdemo.md:174
msgid "Python HTTP API"
msgstr "Python HTTP API"

#: ../../source/demo/webdemo.md:176
msgid "More platform support (Linux, Windows) coming soon."
msgstr "更多平台支持（Linux、Windows）即将推出。"

#: ../../source/demo/webdemo.md:178
msgid "Technical Highlights"
msgstr "技术亮点"

#: ../../source/demo/webdemo.md:180
msgid "**WebRTC Protocol**: Industry-standard real-time communication"
msgstr "**WebRTC 协议**：行业标准的实时通信"

#: ../../source/demo/webdemo.md:181
msgid "**Full-Duplex Architecture**: Simultaneous bidirectional streaming"
msgstr "**全双工架构**：同时双向流传输"

#: ../../source/demo/webdemo.md:182
msgid ""
"**Native llamacpp-omni Support**: Seamlessly integrates with "
"[llamacpp-omni](https://github.com/tc-mb/llama.cpp-omni) as the inference "
"backend"
msgstr ""
"**原生 llamacpp-omni 支持**：与 [llamacpp-omni](https://github.com/tc-mb/llama.cpp-omni) "
"无缝集成作为推理后端"

#: ../../source/demo/webdemo.md:184
msgid "Related Resources"
msgstr "相关资源"
